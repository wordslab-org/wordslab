{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13a90da-823f-4e2b-a008-5053735507d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4825ea4e-0c7c-4b10-ba75-e083197af9d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:05:54.576919Z",
     "iopub.status.busy": "2023-08-23T20:05:54.576632Z",
     "iopub.status.idle": "2023-08-23T20:06:00.891710Z",
     "shell.execute_reply": "2023-08-23T20:06:00.890519Z",
     "shell.execute_reply.started": "2023-08-23T20:05:54.576900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.28.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers->-r requirements.txt (line 1)) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (2023.5.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 1)) (0.14.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/lib/python3/dist-packages (from accelerate->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 2)) (5.9.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->-r requirements.txt (line 1)) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.5)\n",
      "Installing collected packages: bitsandbytes, accelerate\n",
      "Successfully installed accelerate-0.22.0 bitsandbytes-0.41.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54207195-a7d6-4d2a-8ffe-67a9f13c4006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:06:15.265488Z",
     "iopub.status.busy": "2023-08-23T20:06:15.264814Z",
     "iopub.status.idle": "2023-08-23T20:06:15.269808Z",
     "shell.execute_reply": "2023-08-23T20:06:15.268981Z",
     "shell.execute_reply.started": "2023-08-23T20:06:15.265468Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# check transformers version\n",
    "MIN_TRANSFORMERS_VERSION = '4.25.1'\n",
    "assert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n",
    "\n",
    "torch.set_printoptions(precision=2, threshold=3000)\n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364eee30-940b-4dd4-b8e8-c98d7baa723c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:06:16.371753Z",
     "iopub.status.busy": "2023-08-23T20:06:16.371506Z",
     "iopub.status.idle": "2023-08-23T20:06:17.747119Z",
     "shell.execute_reply": "2023-08-23T20:06:17.746483Z",
     "shell.execute_reply.started": "2023-08-23T20:06:16.371737Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fe421-affb-4a3b-9f13-01a1023ab949",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Download and load an open source model\n",
    "\n",
    "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "\n",
    "https://www.together.xyz/blog/redpajama-7b\n",
    "\n",
    "https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bae932-e504-46d7-b033-8a088492c4a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:06:29.375078Z",
     "iopub.status.busy": "2023-08-23T20:06:29.374551Z",
     "iopub.status.idle": "2023-08-23T20:06:42.343185Z",
     "shell.execute_reply": "2023-08-23T20:06:42.342369Z",
     "shell.execute_reply.started": "2023-08-23T20:06:29.375052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "config = AutoConfig.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", device_map='auto', torch_dtype=torch.float16, load_in_8bit=False)\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f39efb4-160e-4f71-a2d5-f9f9335c0f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:07:10.034662Z",
     "iopub.status.busy": "2023-08-23T20:07:10.033839Z",
     "iopub.status.idle": "2023-08-23T20:07:10.041926Z",
     "shell.execute_reply": "2023-08-23T20:07:10.040749Z",
     "shell.execute_reply.started": "2023-08-23T20:07:10.034636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# infer\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def generate_answer(question, max_new_tokens=256):\n",
    "    prompt = f\"<human>: {question}\\n<bot>:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    outputs = model.generate( **inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True)\n",
    "    tokens = outputs.sequences[0, input_length:]\n",
    "    answer = tokenizer.decode(tokens)\n",
    "    return Markdown(\"### Question\\n\" + question + \"\\n### Réponse\\n\" + answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e9e69-162c-45d1-b934-9be6e1f3dee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T20:56:23.646447Z",
     "iopub.status.busy": "2023-06-15T20:56:23.645247Z",
     "iopub.status.idle": "2023-06-15T20:56:23.649969Z",
     "shell.execute_reply": "2023-06-15T20:56:23.649112Z",
     "shell.execute_reply.started": "2023-06-15T20:56:23.646419Z"
    },
    "tags": []
   },
   "source": [
    "### Test the local language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2eb669e-d8ff-46a6-a0bb-d12e87cdaf1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:11:56.635472Z",
     "iopub.status.busy": "2023-08-23T20:11:56.635163Z",
     "iopub.status.idle": "2023-08-23T20:12:13.723454Z",
     "shell.execute_reply": "2023-08-23T20:12:13.722857Z",
     "shell.execute_reply.started": "2023-08-23T20:11:56.635446Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "Comment financer un achat de maison ?\n",
       "### Réponse\n",
       " Pour financer un achat de maison, il faut savoir :\n",
       "\n",
       "- La valeur de la maison : La valeur de la maison dépendra des options et des options de l'achat.\n",
       "\n",
       "- La situation financière : Les revenus et les créances de l'utilisateur doivent être connus pour financer l'achat.\n",
       "\n",
       "- La durée de la dette : La durée de la dette doit être prise en compte pour calculer le taux d'intérêt.\n",
       "\n",
       "- La période de la dette : La période de la dette doit être prise en compte pour calculer le taux d'intérêt.\n",
       "\n",
       "- Les options : Les options peuvent être utilisées pour financer l'achat de la maison, mais il faut savoir les options et les options de l'achat.\n",
       "\n",
       "- Les taxes et impôts : Les taxes et impôts peuvent être facteurs à prendre en compte pour financer l'achat de la maison.\n",
       "\n",
       "- Les coûts : Les coûts de construction et d'achat peuvent"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"Comment financer un achat de maison ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fee8e772-2e66-42a8-a824-e6c54482ba24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T19:36:42.511905Z",
     "iopub.status.busy": "2023-06-11T19:36:42.511626Z",
     "iopub.status.idle": "2023-06-11T19:37:01.335123Z",
     "shell.execute_reply": "2023-06-11T19:37:01.334291Z",
     "shell.execute_reply.started": "2023-06-11T19:36:42.511873Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "En python, comment trier les éléments d'un dictionnaire selon leur valeur décroissante ?\n",
       "### Réponse\n",
       " Pour trier les éléments d'un dictionnaire en fonction de leur valeur décroissante, vous pouvez utiliser le module `sorted` du module `collections`.\n",
       "\n",
       "Voici un exemple de code qui utilise `sorted` pour trier les éléments d'un dictionnaire en fonction de leur valeur décroissante :\n",
       "\n",
       "```python\n",
       "def sort_dict(dictionary):\n",
       "    return sorted(dictionary.sort(key=lambda x: x[1]))\n",
       "```\n",
       "\n",
       "Vous pouvez utiliser ce module pour trier les éléments d'un dictionnaire en fonction de leur valeur décroissante en utilisant la fonction `sort` et en spécifiant les valeurs à trier.\n",
       "\n",
       "Voici un exemple de code qui utilise `sort_dict` pour trier les éléments d'un dictionnaire en fonction de leur valeur décroissante :\n",
       "\n",
       "```python\n",
       "import sorted\n",
       "\n",
       "# Définir le dictionnaire\n",
       "dictionary = {\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"En python, comment trier les éléments d'un dictionnaire selon leur valeur décroissante ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a907de-1183-4451-b37c-8328aefdf593",
   "metadata": {},
   "source": [
    "### Source code of the model : \n",
    "\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a55ba11f-fa5b-4066-b147-1183cc19880c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:15:17.672276Z",
     "iopub.status.busy": "2023-08-23T20:15:17.671705Z",
     "iopub.status.idle": "2023-08-23T20:15:17.676575Z",
     "shell.execute_reply": "2023-08-23T20:15:17.675560Z",
     "shell.execute_reply.started": "2023-08-23T20:15:17.672235Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Comment financer un achat de maison ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa340a-d25e-4cee-871e-0f4472acac02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T21:36:38.114536Z",
     "iopub.status.busy": "2023-06-15T21:36:38.114230Z",
     "iopub.status.idle": "2023-06-15T21:36:38.119121Z",
     "shell.execute_reply": "2023-06-15T21:36:38.118115Z",
     "shell.execute_reply.started": "2023-06-15T21:36:38.114516Z"
    },
    "tags": []
   },
   "source": [
    "#### 1. Tokenizer - Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e5bd02-94cc-4135-b5aa-f0ee504f56a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:15:19.018558Z",
     "iopub.status.busy": "2023-08-23T20:15:19.018152Z",
     "iopub.status.idle": "2023-08-23T20:15:19.065714Z",
     "shell.execute_reply": "2023-08-23T20:15:19.064731Z",
     "shell.execute_reply.started": "2023-08-23T20:15:19.018532Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50277"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4392ea46-50cf-43b6-83a6-0c13b8a60593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:15:27.103760Z",
     "iopub.status.busy": "2023-08-23T20:15:27.102699Z",
     "iopub.status.idle": "2023-08-23T20:15:27.157253Z",
     "shell.execute_reply": "2023-08-23T20:15:27.156465Z",
     "shell.execute_reply.started": "2023-08-23T20:15:27.103737Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 -> ' busy'\n",
      "10001 -> '$_'\n",
      "10002 -> 'who'\n",
      "10003 -> ' institutions'\n",
      "10004 -> 'Select'\n",
      "10005 -> ' progression'\n",
      "10006 -> ' modes'\n",
      "10007 -> ' modify'\n",
      "10008 -> 'description'\n",
      "10009 -> ' disag'\n",
      "10010 -> 'ctic'\n",
      "10011 -> ' SC'\n",
      "10012 -> ' theorem'\n",
      "10013 -> ' grab'\n",
      "10014 -> ' theme'\n",
      "10015 -> ' clothes'\n",
      "10016 -> '2002'\n",
      "10017 -> ' replied'\n",
      "10018 -> ' observe'\n",
      "10019 -> 'iginal'\n",
      "10020 -> ' semi'\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in sorted(tokenizer.vocab.items(), key=lambda x: x[1]):\n",
    "    i += 1\n",
    "    if i<=10000:\n",
    "        continue\n",
    "    print(f\"{item[1]} -> '{item[0].replace('Ġ',' ')}'\")\n",
    "    if i>10020:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d1e13-1940-452c-8424-d2fca2a021a0",
   "metadata": {},
   "source": [
    "#### 2. Tokenizer - Tokens and token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18029e98-f24c-4de9-84e7-a6d2ace01476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:15:39.793949Z",
     "iopub.status.busy": "2023-08-23T20:15:39.793673Z",
     "iopub.status.idle": "2023-08-23T20:15:39.799865Z",
     "shell.execute_reply": "2023-08-23T20:15:39.798728Z",
     "shell.execute_reply.started": "2023-08-23T20:15:39.793932Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXTokenizerFast(name_or_path='togethercomputer/RedPajama-INCITE-Chat-3B-v1', vocab_size=50254, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "956b3e3a-54b3-44cc-8d1a-4672ef5e5fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:15:41.210981Z",
     "iopub.status.busy": "2023-08-23T20:15:41.210735Z",
     "iopub.status.idle": "2023-08-23T20:15:41.215821Z",
     "shell.execute_reply": "2023-08-23T20:15:41.215055Z",
     "shell.execute_reply.started": "2023-08-23T20:15:41.210960Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokens(question):\n",
    "    tokens_ids = tokenizer(question)[\"input_ids\"]\n",
    "    return \"[\" + \"] [\".join([tokenizer.decode(token_id) for token_id in tokens_ids]) + \"]\"\n",
    "    \n",
    "def get_token_ids(question):\n",
    "    tokens_ids = tokenizer(question)[\"input_ids\"]\n",
    "    return \"[\" + \"] [\".join([str(token_id) for token_id in tokens_ids]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7912ddf1-8ab9-47b3-b41e-f058115b0218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:15:41.990928Z",
     "iopub.status.busy": "2023-08-23T20:15:41.990088Z",
     "iopub.status.idle": "2023-08-23T20:15:41.996396Z",
     "shell.execute_reply": "2023-08-23T20:15:41.995327Z",
     "shell.execute_reply.started": "2023-08-23T20:15:41.990844Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Comment] [ fin] [ancer] [ un] [ a] [chat] [ de] [ ma] [ison] [?]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88db85ae-9a00-4644-8f95-795c520ca306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:16:01.978179Z",
     "iopub.status.busy": "2023-08-23T20:16:01.977811Z",
     "iopub.status.idle": "2023-08-23T20:16:01.984089Z",
     "shell.execute_reply": "2023-08-23T20:16:01.983089Z",
     "shell.execute_reply.started": "2023-08-23T20:16:01.978148Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[19174] [1442] [21955] [440] [247] [23481] [372] [6429] [1988] [3736]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_ids(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a52fb-ccd9-46b9-9ffe-6b581c4d5f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T21:43:09.595572Z",
     "iopub.status.busy": "2023-06-15T21:43:09.595217Z",
     "iopub.status.idle": "2023-06-15T21:43:09.601999Z",
     "shell.execute_reply": "2023-06-15T21:43:09.600780Z",
     "shell.execute_reply.started": "2023-06-15T21:43:09.595524Z"
    },
    "tags": []
   },
   "source": [
    "#### 3. Embeddings - Token representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3af073bd-a102-4a87-9de0-e27ab98653fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:16:29.999272Z",
     "iopub.status.busy": "2023-08-23T20:16:29.999022Z",
     "iopub.status.idle": "2023-08-23T20:16:30.006601Z",
     "shell.execute_reply": "2023-08-23T20:16:30.005777Z",
     "shell.execute_reply.started": "2023-08-23T20:16:29.999255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50432, 2560)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt_neox.embed_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d799f783-791f-40b4-b74c-49559fefd996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:17:05.524340Z",
     "iopub.status.busy": "2023-08-23T20:17:05.523590Z",
     "iopub.status.idle": "2023-08-23T20:17:05.529307Z",
     "shell.execute_reply": "2023-08-23T20:17:05.528382Z",
     "shell.execute_reply.started": "2023-08-23T20:17:05.524311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(question):\n",
    "    input_ids = torch.tensor(tokenizer(question)[\"input_ids\"]).cuda()\n",
    "    return model.gpt_neox.embed_in(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "845563e4-c701-4e77-9aa2-3815d5b17be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:17:06.020102Z",
     "iopub.status.busy": "2023-08-23T20:17:06.019423Z",
     "iopub.status.idle": "2023-08-23T20:17:06.026120Z",
     "shell.execute_reply": "2023-08-23T20:17:06.024679Z",
     "shell.execute_reply.started": "2023-08-23T20:17:06.020081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_embeddings = get_embeddings(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1700198f-d414-40be-8073-a8e67954e210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:17:08.797534Z",
     "iopub.status.busy": "2023-08-23T20:17:08.796460Z",
     "iopub.status.idle": "2023-08-23T20:17:08.802522Z",
     "shell.execute_reply": "2023-08-23T20:17:08.801639Z",
     "shell.execute_reply.started": "2023-08-23T20:17:08.797505Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2560])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4aaad6e-42e5-44b4-9fe8-df104b129b9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:17:11.804914Z",
     "iopub.status.busy": "2023-08-23T20:17:11.804677Z",
     "iopub.status.idle": "2023-08-23T20:17:11.837419Z",
     "shell.execute_reply": "2023-08-23T20:17:11.836664Z",
     "shell.execute_reply.started": "2023-08-23T20:17:11.804898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.01, -0.01, -0.02, -0.02,  0.04, -0.01, -0.03, -0.01, -0.03,  0.01,\n",
       "        -0.01, -0.00,  0.02, -0.01,  0.04,  0.02,  0.05,  0.04, -0.01,  0.01,\n",
       "         0.01, -0.01, -0.02, -0.03,  0.00,  0.04,  0.00,  0.02, -0.04,  0.00,\n",
       "        -0.07, -0.01, -0.01,  0.03, -0.02, -0.03,  0.00,  0.01, -0.01, -0.02,\n",
       "         0.02,  0.01, -0.02,  0.03,  0.00,  0.02,  0.01,  0.01, -0.01, -0.01,\n",
       "         0.01, -0.01,  0.01,  0.03,  0.02,  0.03,  0.01, -0.03, -0.01,  0.00,\n",
       "         0.03,  0.04,  0.03,  0.02, -0.01, -0.01,  0.01, -0.02, -0.00, -0.01,\n",
       "         0.00,  0.01,  0.02,  0.03,  0.01, -0.02,  0.00, -0.04,  0.01, -0.02,\n",
       "         0.02,  0.04, -0.03,  0.01,  0.01,  0.03, -0.00, -0.02, -0.00,  0.01,\n",
       "        -0.02,  0.01,  0.01,  0.02,  0.03,  0.02, -0.02,  0.02,  0.03,  0.01,\n",
       "        -0.01,  0.00, -0.00,  0.02,  0.01, -0.00,  0.03, -0.00,  0.01,  0.03,\n",
       "        -0.00,  0.01, -0.01, -0.01, -0.02, -0.00, -0.01,  0.02, -0.02, -0.02,\n",
       "        -0.02, -0.02,  0.01, -0.01,  0.02, -0.01, -0.02,  0.00, -0.00, -0.01,\n",
       "         0.02, -0.03,  0.03,  0.01,  0.01,  0.01,  0.02, -0.00, -0.00,  0.03,\n",
       "         0.01, -0.01,  0.00, -0.01, -0.02, -0.02, -0.00, -0.01,  0.01,  0.01,\n",
       "        -0.04,  0.00, -0.05, -0.00, -0.01,  0.01, -0.01, -0.00, -0.01, -0.01,\n",
       "         0.01, -0.02, -0.00, -0.04, -0.00,  0.04,  0.02,  0.03, -0.00,  0.02,\n",
       "         0.02, -0.01, -0.00,  0.01, -0.02, -0.02, -0.01, -0.00,  0.01,  0.00,\n",
       "         0.00, -0.00,  0.00, -0.02,  0.00, -0.03, -0.01,  0.02, -0.01,  0.03,\n",
       "        -0.00, -0.01, -0.00,  0.01,  0.01,  0.01,  0.00, -0.01, -0.00, -0.03],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings[0,:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480fcfa-1ca3-4f91-a5a6-79ccbb20911b",
   "metadata": {},
   "source": [
    "#### 4. Transformer Layers - Contextual & semantic representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e472635-7dc6-499d-b9f5-053da683c0b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:18:29.470929Z",
     "iopub.status.busy": "2023-08-23T20:18:29.470293Z",
     "iopub.status.idle": "2023-08-23T20:18:29.475839Z",
     "shell.execute_reply": "2023-08-23T20:18:29.475087Z",
     "shell.execute_reply.started": "2023-08-23T20:18:29.470891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "    (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt_neox.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "251b48fe-a014-4b2b-8eef-292e155ef82d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:18:31.746889Z",
     "iopub.status.busy": "2023-08-23T20:18:31.746270Z",
     "iopub.status.idle": "2023-08-23T20:18:31.752814Z",
     "shell.execute_reply": "2023-08-23T20:18:31.752010Z",
     "shell.execute_reply.started": "2023-08-23T20:18:31.746860Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "    (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gpt_neox.layers[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cdf7a-92ad-4f24-8293-cba57f4e434e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T22:20:25.672609Z",
     "iopub.status.busy": "2023-06-15T22:20:25.671474Z",
     "iopub.status.idle": "2023-06-15T22:20:25.678640Z",
     "shell.execute_reply": "2023-06-15T22:20:25.677703Z",
     "shell.execute_reply.started": "2023-06-15T22:20:25.672569Z"
    },
    "tags": []
   },
   "source": [
    "#### 5. Language modeling head - Predicting the next token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77196e05-599e-4ae7-bc9d-d9d2e06dcfbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:20:04.465763Z",
     "iopub.status.busy": "2023-08-23T20:20:04.465323Z",
     "iopub.status.idle": "2023-08-23T20:20:04.471325Z",
     "shell.execute_reply": "2023-08-23T20:20:04.470365Z",
     "shell.execute_reply.started": "2023-08-23T20:20:04.465742Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2560, out_features=50432, bias=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7986e5c-ae09-44c4-9dfa-202cf58064c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:20:45.088618Z",
     "iopub.status.busy": "2023-08-23T20:20:45.087926Z",
     "iopub.status.idle": "2023-08-23T20:20:45.092857Z",
     "shell.execute_reply": "2023-08-23T20:20:45.091658Z",
     "shell.execute_reply.started": "2023-08-23T20:20:45.088596Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(question):\n",
    "    input_ids = torch.tensor(tokenizer(question)[\"input_ids\"]).unsqueeze(dim=0).cuda()\n",
    "    return model(input_ids).logits.squeeze().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01e0358d-53d3-4850-9563-75d268516339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:20:45.620758Z",
     "iopub.status.busy": "2023-08-23T20:20:45.620463Z",
     "iopub.status.idle": "2023-08-23T20:20:46.492182Z",
     "shell.execute_reply": "2023-08-23T20:20:46.491243Z",
     "shell.execute_reply.started": "2023-08-23T20:20:45.620730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = torch.softmax(get_predictions(question), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a79a803c-97c9-4dba-94d9-508581cec6ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:20:48.080103Z",
     "iopub.status.busy": "2023-08-23T20:20:48.079329Z",
     "iopub.status.idle": "2023-08-23T20:20:48.086505Z",
     "shell.execute_reply": "2023-08-23T20:20:48.085364Z",
     "shell.execute_reply.started": "2023-08-23T20:20:48.080071Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50432])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a0c3c23-2f40-455c-85f9-ec4316b69453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:20:50.988600Z",
     "iopub.status.busy": "2023-08-23T20:20:50.988334Z",
     "iopub.status.idle": "2023-08-23T20:20:51.000086Z",
     "shell.execute_reply": "2023-08-23T20:20:50.998968Z",
     "shell.execute_reply.started": "2023-08-23T20:20:50.988566Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.55e-02, 1.83e-10, 2.20e-03,  ..., 1.74e-10, 1.39e-10, 1.42e-10],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_prediction = preds[-1,:]\n",
    "next_token_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a2fdabc-0d06-43ef-9f91-4a228e32b482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:23:17.383759Z",
     "iopub.status.busy": "2023-08-23T20:23:17.383285Z",
     "iopub.status.idle": "2023-08-23T20:23:17.867875Z",
     "shell.execute_reply": "2023-08-23T20:23:17.867060Z",
     "shell.execute_reply.started": "2023-08-23T20:23:17.383718Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.75, 0.04, 0.03, 0.01, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       "       device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([  187,     0, 22955,   330,  2070,   313,  3277, 12029,   428,  3905],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_ids = torch.topk(next_token_prediction, k=10)\n",
    "next_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9ba5b94-8a6c-4460-8e19-34302fdf9f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:23:23.912594Z",
     "iopub.status.busy": "2023-08-23T20:23:23.912251Z",
     "iopub.status.idle": "2023-08-23T20:23:23.920972Z",
     "shell.execute_reply": "2023-08-23T20:23:23.920065Z",
     "shell.execute_reply.started": "2023-08-23T20:23:23.912569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '<|endoftext|>',\n",
       " ' Comment',\n",
       " ' C',\n",
       " ' Le',\n",
       " ' (',\n",
       " ' Qu',\n",
       " ' Les',\n",
       " ' -',\n",
       " ' La']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(token_id) for token_id in next_token_ids.indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c47ef9-78d3-4894-8541-299fa18c2258",
   "metadata": {},
   "source": [
    "#### 6. Observe the inner workings of the model - Iterate to sample an answer token by token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45e0b568-e79e-4c77-af3f-0a485dc9b7ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:26:01.115174Z",
     "iopub.status.busy": "2023-08-23T20:26:01.114732Z",
     "iopub.status.idle": "2023-08-23T20:26:01.121113Z",
     "shell.execute_reply": "2023-08-23T20:26:01.120183Z",
     "shell.execute_reply.started": "2023-08-23T20:26:01.115138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'hook_handles' not in vars():\n",
    "    hook_handles = []\n",
    "else:\n",
    "    for hook_handle in hook_handles:\n",
    "        hook_handle.remove()\n",
    "        hook_handles = []\n",
    "\n",
    "def get_input_tokens(inpTensor):\n",
    "    return \"[\" + \"] [\".join([tokenizer.decode(element.item()).replace('\\n',\"\\\\n\") for element in inpTensor]) + \"]\"\n",
    "\n",
    "def get_output_tokens(outTensor):\n",
    "    preds = torch.softmax(outTensor.float(), dim=1)\n",
    "    next_token_ids = torch.topk(preds[-1,:], k=5)\n",
    "    return [tokenizer.decode(token_id) for token_id in next_token_ids.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4666115-8d52-4035-83fc-05638d119654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:27:26.271369Z",
     "iopub.status.busy": "2023-08-23T20:27:26.270998Z",
     "iopub.status.idle": "2023-08-23T20:27:26.276717Z",
     "shell.execute_reply": "2023-08-23T20:27:26.275725Z",
     "shell.execute_reply.started": "2023-08-23T20:27:26.271347Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for hook_handle in hook_handles:\n",
    "    hook_handle.remove()\n",
    "hook_handles = []\n",
    "\n",
    "def print_embed_in(module, input, output):\n",
    "    inpTensor = input[0].squeeze(dim=0)\n",
    "    print(f\">> Input : {get_input_tokens(inpTensor)}\")\n",
    "    \n",
    "def print_embed_out(module, input, output):\n",
    "    outTensor = output[0]\n",
    "    print(f\">> Output:  {get_output_tokens(outTensor)}\")\n",
    "\n",
    "# Register the hook to a specific layer/module\n",
    "hook_handles.append(model.gpt_neox.embed_in.register_forward_hook(print_embed_in))\n",
    "hook_handles.append(model.embed_out.register_forward_hook(print_embed_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9180a0ab-fe3d-4f4f-b4a2-569d87599019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T20:27:31.213669Z",
     "iopub.status.busy": "2023-08-23T20:27:31.213433Z",
     "iopub.status.idle": "2023-08-23T20:27:48.635875Z",
     "shell.execute_reply": "2023-08-23T20:27:48.634662Z",
     "shell.execute_reply.started": "2023-08-23T20:27:31.213653Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Input : [<] [human] [>:] [ Comment] [ fin] [ancer] [ un] [ a] [chat] [ de] [ ma] [ison] [?] [\\n] [<] [bot] [>:]\n",
      ">> Output:  [' Il', ' Pour', ' Vo', ' F', ' V']\n",
      ">> Input : [ Il]\n",
      ">> Output:  [' existe', ' est', ' faut', ' y', ' n']\n",
      ">> Input : [ est]\n",
      ">> Output:  [' important', ' possible', ' diff', ' très', ' sou']\n",
      ">> Input : [ important]\n",
      ">> Output:  [' de', ' d', ' que', ' pour', ',']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' cho', ' comp', ' sav', ' p', ' se']\n",
      ">> Input : [ comp]\n",
      ">> Output:  ['rendre', 'ter', 'rim', 'ren', 'render']\n",
      ">> Input : [rendre]\n",
      ">> Output:  [' les', ' la', ' le', ' comment', ' ce']\n",
      ">> Input : [ les]\n",
      ">> Output:  [' co', ' diffé', ' dé', ' é', ' avant']\n",
      ">> Input : [ diffé]\n",
      ">> Output:  ['rent', 'rences', 'rends', 'ren', 'rens']\n",
      ">> Input : [rent]\n",
      ">> Output:  ['s', 'es', ' types', ' options', ' su']\n",
      ">> Input : [s]\n",
      ">> Output:  [' fact', ' él', ' aspects', ' types', ' co']\n",
      ">> Input : [ fact]\n",
      ">> Output:  ['eurs', 'eur', 'ures', 'urs', 'ure']\n",
      ">> Input : [eurs]\n",
      ">> Output:  [' qui', ' pour', ' à', ' de', ' li']\n",
      ">> Input : [ qui]\n",
      ">> Output:  [' peu', ' influ', ' affect', ' dé', ' inter']\n",
      ">> Input : [ peu]\n",
      ">> Output:  ['vent', 'pl', ' import', ' à', ' so']\n",
      ">> Input : [vent]\n",
      ">> Output:  [' influ', ' affect', ' avoir', ' a', ' être']\n",
      ">> Input : [ affect]\n",
      ">> Output:  ['er', 'é', 'ent', 'e', 'és']\n",
      ">> Input : [er]\n",
      ">> Output:  [' la', ' votre', ' le', ' les', ' l']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' rent', ' ré', ' vi', ' finance', ' val']\n",
      ">> Input : [ vi]\n",
      ">> Output:  ['abil', 'ande', 'ability', 'abl', 'ager']\n",
      ">> Input : [abil]\n",
      ">> Output:  ['ité', 'ite', 'isation', 'it', 'ités']\n",
      ">> Input : [ité]\n",
      ">> Output:  [' financ', ' d', ' de', ' et', ' du']\n",
      ">> Input : [ d]\n",
      ">> Output:  [\"'\", '’', 'une', ' un', ' une']\n",
      ">> Input : [']\n",
      ">> Output:  ['un', 'une', 'ach', 'invest', 'ache']\n",
      ">> Input : [un]\n",
      ">> Output:  [' a', ' invest', ' pro', ' finance', ' pr']\n",
      ">> Input : [ a]\n",
      ">> Output:  ['chat', 'ch', 'jour', 'com', 'just']\n",
      ">> Input : [chat]\n",
      ">> Output:  [' de', ' d', ' immob', '.', ',']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' ma', ' mais', ' la', ' log', ' dem']\n",
      ">> Input : [ ma]\n",
      ">> Output:  ['ison', 'qu', 'ître', 'ît', 'ç']\n",
      ">> Input : [ison]\n",
      ">> Output:  ['.', ',', ' :', ' et', ' pour']\n",
      ">> Input : [.]\n",
      ">> Output:  ['\\n', ' Pour', ' C', ' Il', ' En']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['\\n', 'Il', 'Le', '1', 'La']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['-', '1', 'Pour', 'La', 'Le']\n",
      ">> Input : [1]\n",
      ">> Output:  ['.', ')', '-', ' -', '.-']\n",
      ">> Input : [.]\n",
      ">> Output:  [' V', ' En', ' L', ' Vale', ' La']\n",
      ">> Input : [ -]\n",
      ">> Output:  [' V', ' É', ' L', ' La', ' Vale']\n",
      ">> Input : [ Vale]\n",
      ">> Output:  ['ur', 'urs', 'u', 'ure', 'UR']\n",
      ">> Input : [ur]\n",
      ">> Output:  [' du', ' de', ' :', ' immob', ' intrins']\n",
      ">> Input : [ du]\n",
      ">> Output:  [' terrain', ' pri', ' b', ' sol', ' bien']\n",
      ">> Input : [ terrain]\n",
      ">> Output:  [' :', ':', ' et', ' -', '\\n']\n",
      ">> Input : [ :]\n",
      ">> Output:  [' La', ' Les', ' Le', ' la', ' le']\n",
      ">> Input : [ La]\n",
      ">> Output:  [' val', ' quant', ' tail', ' surface', ' qual']\n",
      ">> Input : [ val]\n",
      ">> Output:  ['eur', 'eurs', 'abil', 'uation', 'able']\n",
      ">> Input : [eur]\n",
      ">> Output:  [' du', ' de', ' d', ' des', ' maxim']\n",
      ">> Input : [ du]\n",
      ">> Output:  [' terrain', ' site', ' sol', ' ter', ' domain']\n",
      ">> Input : [ terrain]\n",
      ">> Output:  [' est', ' peut', ' jou', ' dé', ' et']\n",
      ">> Input : [ est]\n",
      ">> Output:  [' une', ' un', ' importante', ' sou', ' la']\n",
      ">> Input : [ importante]\n",
      ">> Output:  [' car', ' pour', ' dans', ',', ' lors']\n",
      ">> Input : [ car]\n",
      ">> Output:  [' elle', ' il', ' cela', ' c', ' les']\n",
      ">> Input : [ elle]\n",
      ">> Output:  [' peut', ' est', ' dé', ' représ', ' influence']\n",
      ">> Input : [ peut]\n",
      ">> Output:  [' influ', ' être', ' avoir', ' a', ' affect']\n",
      ">> Input : [ être]\n",
      ">> Output:  [' un', ' util', ' la', ' une', ' l']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' princip', ' plus', ' cl', ' base', ' source']\n",
      ">> Input : [ princip]\n",
      ">> Output:  ['ale', 'aut', 'ales', 'aux', 'elle']\n",
      ">> Input : [ale]\n",
      ">> Output:  [' source', ' res', ' ré', ' é', ' dé']\n",
      ">> Input : [ source]\n",
      ">> Output:  [' de', ' d', ' financ', ' des', ' é']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' reven', ' finance', ' ré', ' cr', ' pa']\n",
      ">> Input : [ reven]\n",
      ">> Output:  ['us', 'u', 'ant', 'ure', 'ir']\n",
      ">> Input : [us]\n",
      ">> Output:  [' pour', ' de', ' d', ' du', ' dans']\n",
      ">> Input : [ pour]\n",
      ">> Output:  [' le', ' votre', ' les', ' une', ' un']\n",
      ">> Input : [ votre]\n",
      ">> Output:  [' ma', ' a', ' pro', ' fam', ' nou']\n",
      ">> Input : [ ma]\n",
      ">> Output:  ['ison', 'î', 'qu', 'ître', 'quette']\n",
      ">> Input : [ison]\n",
      ">> Output:  ['.', ' et', ' après', ',', '\\n']\n",
      ">> Input : [.]\n",
      ">> Output:  [' Il', ' Si', '\\n', ' Les', ' La']\n",
      ">> Input : [ Si]\n",
      ">> Output:  [' le', ' la', ' votre', ' vous', ' elle']\n",
      ">> Input : [ le]\n",
      ">> Output:  [' terrain', ' site', ' sol', ' lot', ' ter']\n",
      ">> Input : [ terrain]\n",
      ">> Output:  [' est', ' n', ' a', ' sur', ' co']\n",
      ">> Input : [ est]\n",
      ">> Output:  [' bien', ' peu', ' important', ' très', ' é']\n",
      ">> Input : [ bien]\n",
      ">> Output:  [' é', ' situ', ' exp', ' en', ' plant']\n",
      ">> Input : [ situ]\n",
      ">> Output:  ['é', 'ée', 'és', '�', 'able']\n",
      ">> Input : [é]\n",
      ">> Output:  [',', ' et', ' à', ' dans', ' sur']\n",
      ">> Input : [,]\n",
      ">> Output:  [' il', ' vous', ' on', ' cela', ' elle']\n",
      ">> Input : [ il]\n",
      ">> Output:  [' peut', ' est', ' y', ' pour', ' sera']\n",
      ">> Input : [ peut]\n",
      ">> Output:  [' être', ' avoir', ' augment', ' également', ' y']\n",
      ">> Input : [ être]\n",
      ">> Output:  [' plus', ' pré', ' possible', ' fac', ' int']\n",
      ">> Input : [ plus]\n",
      ">> Output:  [' fac', ' diff', ' int', ' a', ' avant']\n",
      ">> Input : [ fac]\n",
      ">> Output:  ['ile', 'il', 'iles', 'ilis', 'in']\n",
      ">> Input : [ile]\n",
      ">> Output:  [' de', ' à', ' d', 'ment', ' pour']\n",
      ">> Input : [ à]\n",
      ">> Output:  [' g', ' vend', ' trou', ' faire', ' constru']\n",
      ">> Input : [ g]\n",
      ">> Output:  ['é', 'arer', 'agner', 'ager', 'ros']\n",
      ">> Input : [é]\n",
      ">> Output:  ['rer', 'ol', 'oler', 'om', 'or']\n",
      ">> Input : [rer]\n",
      ">> Output:  [' et', '.', ',', ' en', ' que']\n",
      ">> Input : [ et]\n",
      ">> Output:  [' à', ' plus', ' moins', ' donc', ' peut']\n",
      ">> Input : [ à]\n",
      ">> Output:  [' g', ' am', ' ent', ' met', ' faire']\n",
      ">> Input : [ am]\n",
      ">> Output:  ['é', 'én', 'ener', 'asser', 'ort']\n",
      ">> Input : [é]\n",
      ">> Output:  ['li', 'ler', 'ner', 'lier', 'rican']\n",
      ">> Input : [li]\n",
      ">> Output:  ['orer', 'ore', 'or', 'oration', 'orent']\n",
      ">> Input : [orer]\n",
      ">> Output:  ['.', ',', ' en', ' pour', ' plus']\n",
      ">> Input : [.]\n",
      ">> Output:  ['\\n', ' Si', ' Mais', ' Il', ' Les']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['\\n', '2', '-', ' -', '    ']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['2', '-', '1', '3', '•']\n",
      ">> Input : [2]\n",
      ">> Output:  ['.', ' -', '.-', '-', '.']\n",
      ">> Input : [.]\n",
      ">> Output:  [' -', '  ', ' Tax', ' D', ' L']\n",
      ">> Input : [ -]\n",
      ">> Output:  [' É', ' Sit', ' Prix', ' Am', ' D']\n",
      ">> Input : [ Prix]\n",
      ">> Output:  [' du', ' de', ' des', ' immob', ' d']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' l', ' la', ' vent', ' location', ' rev']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' ma', ' location', ' construction', ' propri', ' ter']\n",
      ">> Input : [ location]\n",
      ">> Output:  [' :', ' de', ' du', ':', ' et']\n",
      ">> Input : [ :]\n",
      ">> Output:  [' La', ' Les', ' Le', ' Si', ' L']\n",
      ">> Input : [ La]\n",
      ">> Output:  [' location', ' val', ' rent', ' ré', ' situation']\n",
      ">> Input : [ location]\n",
      ">> Output:  [' de', ' d', ' du', ' des', ' peut']\n",
      ">> Input : [ d]\n",
      ">> Output:  [\"'\", '’', \"'''\", 'ure', 'ite']\n",
      ">> Input : [']\n",
      ">> Output:  ['un', 'une', 'es', 'imme', 'autres']\n",
      ">> Input : [un]\n",
      ">> Output:  [' terrain', ' lot', ' es', ' app', ' log']\n",
      ">> Input : [ terrain]\n",
      ">> Output:  [' peut', ' est', ' à', ' pour', ' sur']\n",
      ">> Input : [ peut]\n",
      ">> Output:  [' être', ' également', ' augment', ' aussi', ' avoir']\n",
      ">> Input : [ être]\n",
      ">> Output:  [' un', ' une', ' plus', ' très', ' moins']\n",
      ">> Input : [ un]\n",
      ">> Output:  [' fact', ' invest', ' m', ' avant', ' co']\n",
      ">> Input : [ fact]\n",
      ">> Output:  ['eur', 'eurs', 'ure', 'age', 'uel']\n",
      ">> Input : [eur]\n",
      ">> Output:  [' important', ' à', ' pour', ' de', ' cl']\n",
      ">> Input : [ important]\n",
      ">> Output:  [' pour', ' car', ',', '.', ' si']\n",
      ">> Input : [ pour]\n",
      ">> Output:  [' les', ' la', ' le', ' votre', ' l']\n",
      ">> Input : [ les]\n",
      ">> Output:  [' invest', ' a', ' co', ' budgets', ' propri']\n",
      ">> Input : [ invest]\n",
      ">> Output:  ['isse', 'iss', 'eurs', 'is', 'itures']\n",
      ">> Input : [isse]\n",
      ">> Output:  ['urs', 'ments', 'ment', 'ur', 'ures']\n",
      ">> Input : [urs]\n",
      ">> Output:  ['.', ',', ' qui', ' nov', ' en']\n",
      ">> Input : [.]\n",
      ">> Output:  [' Les', ' Si', ' Il', ' La', ' Le']\n",
      ">> Input : [ Il]\n",
      ">> Output:  [' est', ' peut', ' faut', ' y', ' existe']\n",
      ">> Input : [ peut]\n",
      ">> Output:  [' être', ' y', ' ar', ' ex', ' s']\n",
      ">> Input : [ être]\n",
      ">> Output:  [' plus', ' pré', ' important', ' int', ' diff']\n",
      ">> Input : [ plus]\n",
      ">> Output:  [' fac', ' diff', ' é', ' co', ' cher']\n",
      ">> Input : [ fac]\n",
      ">> Output:  ['ile', 'il', 'iles', 'ilis', 'in']\n",
      ">> Input : [ile]\n",
      ">> Output:  [' de', ' à', ' d', ' pour', 'ment']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' lou', ' g', ' trou', ' vend', ' pay']\n",
      ">> Input : [ lou]\n",
      ">> Output:  ['er', 'ir', 'cher', 'rer', 'e']\n",
      ">> Input : [er]\n",
      ">> Output:  [' un', ' une', ' des', ' la', ' votre']\n",
      ">> Input : [ une]\n",
      ">> Output:  [' ma', ' propri', ' part', ' location', ' surface']\n",
      ">> Input : [ ma]\n",
      ">> Output:  ['ison', 'quette', 'ç', 'qu', 'ISON']\n",
      ">> Input : [ison]\n",
      ">> Output:  [' si', ' à', ' sur', ' en', ' dans']\n",
      ">> Input : [ si]\n",
      ">> Output:  [' le', ' elle', ' la', ' vous', ' les']\n",
      ">> Input : [ le]\n",
      ">> Output:  [' terrain', ' pri', ' propri', ' march', ' site']\n",
      ">> Input : [ terrain]\n",
      ">> Output:  [' est', ' se', ' n', ' co', ' a']\n",
      ">> Input : [ est]\n",
      ">> Output:  [' situ', ' bien', ' pro', ' local', ' à']\n",
      ">> Input : [ bien]\n",
      ">> Output:  [' situ', ' position', ' local', ' plac', ' é']\n",
      ">> Input : [ situ]\n",
      ">> Output:  ['é', 'ée', 'és', 'able', 'ant']\n",
      ">> Input : [é]\n",
      ">> Output:  [' et', '.', ',', ' à', ' en']\n",
      ">> Input : [.]\n",
      ">> Output:  ['\\n', ' Les', ' C', ' Il', ' Si']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['\\n', '3', '<', '2', '4']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['3', '4', '2', '-', '1']\n",
      ">> Input : [3]\n",
      ">> Output:  ['.', ' -', '-', '.-', ',']\n",
      ">> Input : [.]\n",
      ">> Output:  [' -', '  ', ' D', ' É', ' Tax']\n",
      ">> Input : [ -]\n",
      ">> Output:  [' É', ' Prix', ' Am', ' D', ' Invest']\n",
      ">> Input : [ Cr]\n",
      ">> Output:  ['é', 'éd', 'ise', 'oup', 'ude']\n",
      ">> Input : [é]\n",
      ">> Output:  ['ation', 'er', 'os', 'ne', 'anc']\n",
      ">> Input : [er]\n",
      ">> Output:  [' des', ' un', ' une', ' du', ' de']\n",
      ">> Input : [ un]\n",
      ">> Output:  [' pat', ' port', ' es', ' reven', ' budget']\n",
      ">> Input : [ pat]\n",
      ">> Output:  ['rim', 'ro', 'in', 'roc', 'ino']\n",
      ">> Input : [rim]\n",
      ">> Output:  ['o', 'onial', 'om', 'on', 'oi']\n",
      ">> Input : [o]\n",
      ">> Output:  ['ine', 'ines', 'INE', 'inel', 'inee']\n",
      ">> Input : [ine]\n",
      ">> Output:  [' :', ' durable', ' immob', ' pour', ' à']\n",
      ">> Input : [ :]\n",
      ">> Output:  [' La', ' Une', ' Un', ' Le', ' Si']\n",
      ">> Input : [ Le]\n",
      ">> Output:  [' pat', ' but', ' fait', ' plus', 'ur']\n",
      ">> Input : [ pat]\n",
      ">> Output:  ['rim', 'rimp', 'ron', 'ro', 'rone']\n",
      ">> Input : [rim]\n",
      ">> Output:  ['o', 'oi', 'onial', 'om', 'ony']\n",
      ">> Input : [o]\n",
      ">> Output:  ['ine', 'in', 'ining', 'ins', 'inal']\n",
      ">> Input : [ine]\n",
      ">> Output:  [' est', ' d', ' de', ' peut', ' cr']\n",
      ">> Input : [ est]\n",
      ">> Output:  [' important', ' un', ' une', ' l', ' ess']\n",
      ">> Input : [ important]\n",
      ">> Output:  [' pour', ' car', ',', ' puis', ' si']\n",
      ">> Input : [ pour]\n",
      ">> Output:  [' les', ' la', ' ass', ' garant', ' l']\n",
      ">> Input : [ les]\n",
      ">> Output:  [' invest', ' g', ' person', ' propri', ' fam']\n",
      ">> Input : [ invest]\n",
      ">> Output:  ['isse', 'iss', 'itures', 'is', 'eurs']\n",
      ">> Input : [isse]\n",
      ">> Output:  ['urs', 'ments', 'ur', 'ment', 'ures']\n",
      ">> Input : [urs]\n",
      ">> Output:  [' car', ',', '.', ' qui', ' pour']\n",
      ">> Input : [ car]\n",
      ">> Output:  [' il', ' ils', ' cela', ' c', ' ça']\n",
      ">> Input : [ il]\n",
      ">> Output:  [' peut', ' per', ' est', ' leur', ' cr']\n",
      ">> Input : [ peut]\n",
      ">> Output:  [' être', ' perm', ' a', ' am', ' vous']\n",
      ">> Input : [ perm]\n",
      ">> Output:  ['ett', 'é', 'iser', 'is', 'uter']\n",
      ">> Input : [ett]\n",
      ">> Output:  ['re', 'ent', 'ager', 'ant', 'rir']\n",
      ">> Input : [re]\n",
      ">> Output:  [' d', ' de', ' aux', ' à', ' la']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' g', ' géné', ' cr', ' fin', ' cons']\n",
      ">> Input : [ g]\n",
      ">> Output:  ['é', 'agner', 'ère', 'ér', 'ros']\n",
      ">> Input : [é]\n",
      ">> Output:  ['rer', 'rait', 'rant', 'ra', 're']\n",
      ">> Input : [rer]\n",
      ">> Output:  [' les', ' plus', ' la', ' des', ' l']\n",
      ">> Input : [ les]\n",
      ">> Output:  [' co', ' invest', ' dé', ' charges', ' ris']\n",
      ">> Input : [ co]\n",
      ">> Output:  ['û', '-', 'invest', ' invest', ' investments']\n",
      ">> Input : [û]\n",
      ">> Output:  ['ts', 't', 'tes', 'te', 'TS']\n",
      ">> Input : [ts]\n",
      ">> Output:  [' de', ' d', ' et', ' plus', ' li']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' la', ' construction', ' maintenance', ' maint', ' l']\n",
      ">> Input : [ construction]\n",
      ">> Output:  [' et', ' plus', ' de', '.', ',']\n",
      ">> Input : [ et]\n",
      ">> Output:  [' d', ' de', ' les', ' la', ' des']\n",
      ">> Input : [ d]\n",
      ">> Output:  [\"'\", '’', 'autres', \"'''\", '\\'\"']\n",
      ">> Input : [']\n",
      ">> Output:  ['ent', 'am', 'ach', 'invest', 'ac']\n",
      ">> Input : [ent]\n",
      ">> Output:  ['ret', 'ra', 'repos', 'rain', 'rep']\n",
      ">> Input : [ret]\n",
      ">> Output:  ['ien', 'en', 'iens', 'ient', 'enance']\n",
      ">> Input : [ien]\n",
      ">> Output:  [' plus', ' de', '.', ' d', ' en']\n",
      ">> Input : [ plus]\n",
      ">> Output:  [' fac', ' a', ' long', ' effic', ' tard']\n",
      ">> Input : [ a]\n",
      ">> Output:  ['is', 'ise', 'just', 'ér', 'ch']\n",
      ">> Input : [is]\n",
      ">> Output:  ['ément', 'ement', 'és', 'é', 'ements']\n",
      ">> Input : [ément]\n",
      ">> Output:  ['.', ' et', ',', ' en', ' tout']\n",
      ">> Input : [.]\n",
      ">> Output:  ['\\n', ' Il', ' Si', ' Les', ' C']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['\\n', '4', '<', '3', '    ']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['4', 'Il', '5', 'En', '3']\n",
      ">> Input : [4]\n",
      ">> Output:  ['.', ' -', '.-', '.', ',']\n",
      ">> Input : [.]\n",
      ">> Output:  [' -', ' –', '  ', ' D', ' É']\n",
      ">> Input : [ -]\n",
      ">> Output:  [' Cr', ' Invest', ' É', ' D', ' Am']\n",
      ">> Input : [ Dur]\n",
      ">> Output:  ['ée', 'abil', 'ées', 'ant', 'é']\n",
      ">> Input : [ée]\n",
      ">> Output:  [' de', ' du', ' d', ' et', ' des']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' la', ' vie', ' v', ' l', ' dur']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' construction', ' cr', ' vent', ' location', ' dur']\n",
      ">> Input : [ construction]\n",
      ">> Output:  [' :', ' et', ':', ' de', ' du']\n",
      ">> Input : [ :]\n",
      ">> Output:  [' La', ' Les', ' Le', ' Il', ' L']\n",
      ">> Input : [ La]\n",
      ">> Output:  [' dur', ' construction', ' long', ' tail', ' ré']\n",
      ">> Input : [ dur]\n",
      ">> Output:  ['ée', 'abil', 'ées', 'é', 'ata']\n",
      ">> Input : [ée]\n",
      ">> Output:  [' de', ' du', ' d', ' tot', ' des']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' la', ' construction', ' l', ' fabrication', ' ré']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' construction', ' cr', ' conception', ' ré', ' fabrication']\n",
      ">> Input : [ construction]\n",
      ">> Output:  [' peut', ' d', ' est', ' de', ' dé']\n",
      ">> Input : [ peut]\n",
      ">> Output:  [' être', ' également', ' avoir', ' affect', ' var']\n",
      ">> Input : [ être]\n",
      ">> Output:  [' un', ' une', ' long', ' importante', ' important']\n",
      ">> Input : [ un]\n",
      ">> Output:  [' fact', ' autre', ' point', ' él', ' probl']\n",
      ">> Input : [ fact]\n",
      ">> Output:  ['eur', 'eurs', 'ure', 'uel', 'age']\n",
      ">> Input : [eur]\n",
      ">> Output:  [' important', ' pour', ' importante', ' qui', ' dé']\n",
      ">> Input : [ important]\n",
      ">> Output:  [' pour', ' car', ' si', '.', ',']\n",
      ">> Input : [ pour]\n",
      ">> Output:  [' les', ' cert', ' le', ' certain', ' ce']\n",
      ">> Input : [ les]\n",
      ">> Output:  [' invest', ' investors', ' g', ' nou', ' person']\n",
      ">> Input : [ invest]\n",
      ">> Output:  ['isse', 'is', 'iss', 'eurs', 'itures']\n",
      ">> Input : [isse]\n",
      ">> Output:  ['urs', 'ments', 'ur', 'ment', 'ures']\n",
      ">> Input : [urs]\n",
      ">> Output:  [' car', '.', ',', ' qui', ' pour']\n",
      ">> Input : [ car]\n",
      ">> Output:  [' elle', ' il', ' ils', ' cela', ' les']\n",
      ">> Input : [ elle]\n",
      ">> Output:  [' peut', ' est', ' dé', ' affect', ' a']\n",
      ">> Input : [ peut]\n",
      ">> Output:  [' affect', ' être', ' influ', ' avoir', ' a']\n",
      ">> Input : [ affect]\n",
      ">> Output:  ['er', 'é', 'ent', 'ER', 'ée']\n",
      ">> Input : [er]\n",
      ">> Output:  [' la', ' le', ' les', ' leur', ' l']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' rent', ' vi', ' ré', ' stabil', ' disp']\n",
      ">> Input : [ rent]\n",
      ">> Output:  ['abil', 'ability', 'ention', 'abl', 'ab']\n",
      ">> Input : [abil]\n",
      ">> Output:  ['ité', 'ite', 'it', 'isation', 'ités']\n",
      ">> Input : [ité]\n",
      ">> Output:  [' de', ' du', ' d', ' des', ' et']\n",
      ">> Input : [ de]\n",
      ">> Output:  [' la', ' l', ' leur', ' leurs', ' votre']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' ma', ' construction', ' propri', ' transaction', ' vent']\n",
      ">> Input : [ construction]\n",
      ">> Output:  ['.', ' et', ' plus', ' en', ' d']\n",
      ">> Input : [.]\n",
      ">> Output:  ['\\n', ' Il', ' Les', ' Si', ' La']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['\\n', '<', '5', '4', '-']\n",
      ">> Input : [\\n]\n",
      ">> Output:  ['5', 'Il', 'En', '4', 'Les']\n",
      ">> Input : [5]\n",
      ">> Output:  ['.', ' -', '.', '.-', '-']\n",
      ">> Input : [.]\n",
      ">> Output:  [' -', ' Invest', ' La', ' É', ' L']\n",
      ">> Input : [ -]\n",
      ">> Output:  [' É', ' Invest', ' Tax', ' Com', ' Am']\n",
      ">> Input : [ Tax]\n",
      ">> Output:  ['es', 'e', 'ations', 'ation', 'at']\n",
      ">> Input : [es]\n",
      ">> Output:  [' :', ' sur', ' fon', ' et', ' immob']\n",
      ">> Input : [ :]\n",
      ">> Output:  [' Les', ' La', ' Tax', ' L', ' Il']\n",
      ">> Input : [ Les]\n",
      ">> Output:  [' taxes', ' t', ' imp', ' tax', ' fra']\n",
      ">> Input : [ taxes]\n",
      ">> Output:  [' sur', ' peu', ' sont', ' et', ' pour']\n",
      ">> Input : [ sur]\n",
      ">> Output:  [' les', ' la', ' le', ' l', ' votre']\n",
      ">> Input : [ la]\n",
      ">> Output:  [' location', ' construction', ' vent', ' propri', ' ma']\n",
      ">> Input : [ location]\n",
      ">> Output:  [' d', ' et', ' de', ' ou', ' peu']\n",
      ">> Input : [ d]\n",
      ">> Output:  [\"'\", '’', \"'''\", '\\'\"', 'ud']\n",
      ">> Input : [']\n",
      ">> Output:  ['un', 'une', 'imme', 'imm', 'hab']\n",
      ">> Input : [un]\n",
      ">> Output:  [' terrain', ' bien', ' ter', ' im', ' lot']\n",
      ">> Input : [ terrain]\n",
      ">> Output:  [' peu', ' et', ' sont', ' ou', ' peut']\n",
      ">> Input : [ peu]\n",
      ">> Output:  ['vent', 'ent', ' import', ' peu', ' à']\n",
      ">> Input : [vent]\n",
      ">> Output:  [' être', ' affect', ' augment', ' également', ' avoir']\n",
      ">> Input : [ être]\n",
      ">> Output:  [' important', ' un', ' des', ' une', ' plus']\n",
      ">> Input : [ important]\n",
      ">> Output:  [' pour', 'es', 'e', ' si', ' car']\n",
      ">> Input : [ pour]\n",
      ">> Output:  [' les', ' le', ' cert', ' certain', ' l']\n",
      ">> Input : [ les]\n",
      ">> Output:  [' invest', ' investors', ' tax', ' taxes', ' propri']\n",
      ">> Input : [ invest]\n",
      ">> Output:  ['isse', 'is', 'iss', 'eurs', 'itures']\n",
      ">> Input : [isse]\n",
      ">> Output:  ['urs', 'ments', 'ur', 'ures', 'ment']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Question\n",
       "Comment financer un achat de maison ?\n",
       "### Réponse\n",
       " Il est important de comprendre les différents facteurs qui peuvent affecter la viabilité d'un achat de maison.\n",
       "\n",
       "1. - Valeur du terrain : La valeur du terrain est importante car elle peut être la principale source de revenus pour votre maison. Si le terrain est bien situé, il peut être plus facile à gérer et à améliorer.\n",
       "\n",
       "2. - Prix de la location : La location d'un terrain peut être un facteur important pour les investisseurs. Il peut être plus facile de louer une maison si le terrain est bien situé.\n",
       "\n",
       "3. - Créer un patrimoine : Le patrimoine est important pour les investisseurs car il peut permettre de gérer les coûts de construction et d'entretien plus aisément.\n",
       "\n",
       "4. - Durée de la construction : La durée de la construction peut être un facteur important pour les investisseurs car elle peut affecter la rentabilité de la construction.\n",
       "\n",
       "5. - Taxes : Les taxes sur la location d'un terrain peuvent être important pour les investisseurs"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ece7f-cbca-4fbf-a422-f6f46630389a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redpajama-3b",
   "language": "python",
   "name": "redpajama-3b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
